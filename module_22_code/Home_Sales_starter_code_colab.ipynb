{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/openjdk/11/manifests/11.0.26-1\u001b[0m\n",
      "Already downloaded: /Users/admin/Library/Caches/Homebrew/downloads/6a8d64b799ebc12fb74bf0c886fde0f73187fd5fb76e1504c919a9ee75feb7e8--openjdk@11-11.0.26-1.bottle_manifest.json\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mopenjdk@11\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/openjdk/11/blobs/sha256:1d8776a\u001b[0m\n",
      "Already downloaded: /Users/admin/Library/Caches/Homebrew/downloads/566c48b1de86ea41dd60bd4913a0b063d13a7c7d3e06413c8e089e5d501e2575--openjdk@11--11.0.26.arm64_sequoia.bottle.1.tar.gz\n",
      "\u001b[32m==>\u001b[0m \u001b[1mReinstalling \u001b[32mopenjdk@11\u001b[39m \u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring openjdk@11--11.0.26.arm64_sequoia.bottle.1.tar.gz\u001b[0m\n",
      "ðŸº  /opt/homebrew/Cellar/openjdk@11/11.0.26: 667 files, 295.3MB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup openjdk@11`...\u001b[0m\n",
      "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
      "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   196  100   196    0     0   1889      0 --:--:-- --:--:-- --:--:--  1902\n",
      "tar: Error opening archive: Unrecognized archive format\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pyspark\n",
    "# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.5.3'\n",
    "spark_version = 'spark-3.5.3'\n",
    "os.environ['SPARK_VERSION'] = spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "# Use Homebrew to install OpenJDK 11\n",
    "!brew reinstall openjdk@11\n",
    "\n",
    "# Set JAVA_HOME\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/local/opt/openjdk@11\"\n",
    "\n",
    "# Download Spark using curl\n",
    "!curl -O https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
    "\n",
    "# Extract Spark\n",
    "!tar xf spark-3.5.3-bin-hadoop3.tgz\n",
    "\n",
    "# Install findspark\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"SPARK_HOME\"] = f\"{os.getcwd()}/spark-3.5.3-bin-hadoop3\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "2XbWNf1Te5fM"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/admin/Documents/GitHub/Home_Sales/module_22_code/spark-3.5.3-bin-hadoop3/./bin/spark-submit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create a SparkSession\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSQL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/java_gateway.py:97\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         signal\u001b[38;5;241m.\u001b[39msignal(signal\u001b[38;5;241m.\u001b[39mSIGINT, signal\u001b[38;5;241m.\u001b[39mSIG_IGN)\n\u001b[1;32m     96\u001b[0m     popen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreexec_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m preexec_func\n\u001b[0;32m---> 97\u001b[0m     proc \u001b[38;5;241m=\u001b[39m Popen(command, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpopen_kwargs)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     proc \u001b[38;5;241m=\u001b[39m Popen(command, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpopen_kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[1;32m   1032\u001b[0m                         restore_signals,\n\u001b[1;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:1953\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1951\u001b[0m     err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[1;32m   1952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1955\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/admin/Documents/GitHub/Home_Sales/module_22_code/spark-3.5.3-bin-hadoop3/./bin/spark-submit'"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wOJqxG_RPSwp"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkFiles\n\u001b[1;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39maddFile(url)\n\u001b[1;32m      5\u001b[0m home_sales_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(SparkFiles\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhome_sales_revised.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Read the home_sales_revised.csv from the provided AWS S3 bucket location into a PySpark DataFrame.\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "home_sales_df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RoljcJ7WPpnm"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'home_sales_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 2. Create a temporary view of the DataFrame.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m home_sales_df\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhome_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'home_sales_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 2. Create a temporary view of the DataFrame.\n",
    "home_sales_df.createOrReplaceTempView(\"home_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "L6fkwOeOmqvq"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 3. What is the average price for a four bedroom house sold per year, rounded to two decimal places?\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m avg_price_four_bedroom \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    SELECT YEAR(date) AS year, ROUND(AVG(price), 2) AS avg_price_four_bedroom\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    FROM home_sales\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    WHERE bedrooms = 4\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    GROUP BY year\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m    ORDER BY year\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage price for a four-bedroom house sold per year:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m avg_price_four_bedroom\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 3. What is the average price for a four bedroom house sold per year, rounded to two decimal places?\n",
    "avg_price_four_bedroom = spark.sql(\"\"\"\n",
    "    SELECT YEAR(date) AS year, ROUND(AVG(price), 2) AS avg_price_four_bedroom\n",
    "    FROM home_sales\n",
    "    WHERE bedrooms = 4\n",
    "    GROUP BY year\n",
    "    ORDER BY year\n",
    "\"\"\")\n",
    "print(\"Average price for a four-bedroom house sold per year:\")\n",
    "avg_price_four_bedroom.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "l8p_tUS8h8it"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4. What is the average price of a home for each year the home was built,\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# that have 3 bedrooms and 3 bathrooms, rounded to two decimal places?\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m avg_price_three_bed_bath \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    SELECT date_built, ROUND(AVG(price), 2) AS avg_price_four_bedroom\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    FROM home_sales\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    WHERE bedrooms = 3 AND bathrooms = 3\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m    GROUP BY date_built\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    ORDER BY date_built\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage price of a home for each year built (3 bedrooms, 3 bathrooms):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m avg_price_three_bed_bath\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. What is the average price of a home for each year the home was built,\n",
    "# that have 3 bedrooms and 3 bathrooms, rounded to two decimal places?\n",
    "avg_price_three_bed_bath = spark.sql(\"\"\"\n",
    "    SELECT date_built, ROUND(AVG(price), 2) AS avg_price_four_bedroom\n",
    "    FROM home_sales\n",
    "    WHERE bedrooms = 3 AND bathrooms = 3\n",
    "    GROUP BY date_built\n",
    "    ORDER BY date_built\n",
    "\"\"\")\n",
    "print(\"Average price of a home for each year built (3 bedrooms, 3 bathrooms):\")\n",
    "avg_price_three_bed_bath.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Y-Eytz64liDU"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 5. What is the average price of a home for each year the home was built,\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# that have 3 bedrooms, 3 bathrooms, with two floors,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# and are greater than or equal to 2,000 square feet, rounded to two decimal places?\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m avg_price_three_bed_bath_floors \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    SELECT date_built, ROUND(AVG(price), 2) AS avg_price_four_bedroom\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    FROM home_sales\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m    WHERE bedrooms = 3 AND bathrooms = 3 AND floors = 2 AND sqft_living >= 2000\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    GROUP BY date_built\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m    ORDER BY date_built\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage price of a home for each year built (3 bedrooms, 3 bathrooms, 2 floors, >=2000 sqft):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m avg_price_three_bed_bath_floors\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 5. What is the average price of a home for each year the home was built,\n",
    "# that have 3 bedrooms, 3 bathrooms, with two floors,\n",
    "# and are greater than or equal to 2,000 square feet, rounded to two decimal places?\n",
    "avg_price_three_bed_bath_floors = spark.sql(\"\"\"\n",
    "    SELECT date_built, ROUND(AVG(price), 2) AS avg_price_four_bedroom\n",
    "    FROM home_sales\n",
    "    WHERE bedrooms = 3 AND bathrooms = 3 AND floors = 2 AND sqft_living >= 2000\n",
    "    GROUP BY date_built\n",
    "    ORDER BY date_built\n",
    "    \"\"\")\n",
    "print(\"Average price of a home for each year built (3 bedrooms, 3 bathrooms, 2 floors, >=2000 sqft):\")\n",
    "avg_price_three_bed_bath_floors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUrfgOX1pCRd",
    "outputId": "0a645986-7179-440c-c0bc-345f1693f8af"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 6. What is the average price of a home per \"view\" rating, rounded to two decimal places,\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# having an average home price greater than or equal to $350,000? Order by descending view rating.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Although this is a small dataset, determine the run time for this query.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m avg_price_view_rating \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m    SELECT view, ROUND(AVG(price), 2) AS avg_price\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    FROM home_sales\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m    GROUP BY view\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m    HAVING avg_price >= 350000\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m    ORDER BY view DESC\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage price of a home per \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mview\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m rating(>= $350,000):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m avg_price_view_rating\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 6. What is the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000? Order by descending view rating.\n",
    "# Although this is a small dataset, determine the run time for this query.\n",
    "\n",
    "start_time = time.time()\n",
    "avg_price_view_rating = spark.sql(\"\"\"\n",
    "    SELECT view, ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_sales\n",
    "    GROUP BY view\n",
    "    HAVING avg_price >= 350000\n",
    "    ORDER BY view DESC\n",
    "\"\"\")\n",
    "print(\"Average price of a home per 'view' rating(>= $350,000):\")\n",
    "avg_price_view_rating.show()\n",
    "print(\"---%s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "KAhk3ZD2tFy8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 7. Cache the the temporary table home_sales.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m spark\u001b[38;5;241m.\u001b[39mcatalog\u001b[38;5;241m.\u001b[39mcacheTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhome_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 7. Cache the the temporary table home_sales.\n",
    "spark.catalog.cacheTable(\"home_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4opVhbvxtL-i"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 8. Check if the table is cached.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m is_cached \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcatalog\u001b[38;5;241m.\u001b[39misCached(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhome_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome_sales\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cached?, is_cached\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 8. Check if the table is cached.\n",
    "is_cached = spark.catalog.isCached(\"home_sales\")\n",
    "print(\"Is 'home_sales' cached?, is_cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GnL46lwTSEk",
    "outputId": "63c7dc50-d96a-4a48-97b6-91a446cdb973"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 9. Using the cached data, run the last query above, that calculates\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# the average price of a home per \"view\" rating, rounded to two decimal places,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# having an average home price greater than or equal to $350,000.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Determine the runtime and compare it to the uncached runtime.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 7\u001b[0m avg_price_view_rating_cached \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    SELECT view, ROUND(AVG(price), 2) AS avg_price \u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m    FROM home_sales\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m    GROUP BY view\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m    HAVING avg_price >= 350000\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m    ORDER BY view DESC\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage price of a home per \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mview\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m rating (cached, >= $350,000):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m avg_price_view_rating_cached\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 9. Using the cached data, run the last query above, that calculates\n",
    "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000.\n",
    "# Determine the runtime and compare it to the uncached runtime.\n",
    "\n",
    "start_time = time.time()\n",
    "avg_price_view_rating_cached = spark.sql(\"\"\"\n",
    "    SELECT view, ROUND(AVG(price), 2) AS avg_price \n",
    "    FROM home_sales\n",
    "    GROUP BY view\n",
    "    HAVING avg_price >= 350000\n",
    "    ORDER BY view DESC\n",
    "\"\"\")\n",
    "print(\"Average price of a home per 'view' rating (cached, >= $350,000):\")\n",
    "avg_price_view_rating_cached.show()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Qm12WN9isHBR"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'home_sales_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 10. Partition by the \"date_built\" field on the formatted parquet home sales data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m home_sales_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_built\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhome_sales_partitioned.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'home_sales_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data\n",
    "home_sales_df.write.partitionBy(\"date_built\").parquet(\"home_sales_partitioned.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "AZ7BgY61sRqY"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 11. Read the parquet formatted data.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m home_sales_partitioned_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhome_sales_partitioned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 11. Read the parquet formatted data.\n",
    "home_sales_partitioned_df = spark.read.parquet(\"home_sales_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6MJkHfvVcvh"
   },
   "outputs": [],
   "source": [
    "# 12. Create a temporary table for the parquet data.\n",
    "home_sales_partitioned_df.createOrReplaceTempView(\"home_sales_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_Vhb52rU1Sn",
    "outputId": "d6748ea6-d70a-41fd-dcb8-c214a85e949e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 13. Using the parquet DataFrame, run the last query above, that calculates\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# the average price of a home per \"view\" rating, rounded to two decimal places,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# having an average home price greater than or equal to $350,000.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Determine the runtime and compare it to the cached runtime.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 7\u001b[0m avg_price_view_rating_partitioned \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    SELECT view, ROUND(AVG(price), 2) AS avg_price\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m    FROM home_sales_partitioned\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m    GROUP BY view\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m    HAVING avg_price >= 350000\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m    ORDER BY view DESC\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage price of a home per \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mview\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m rating (partitioned, >= 350,000):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m avg_price_view_rating_partitioned\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 13. Using the parquet DataFrame, run the last query above, that calculates\n",
    "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000.\n",
    "# Determine the runtime and compare it to the cached runtime.\n",
    "\n",
    "start_time = time.time()\n",
    "avg_price_view_rating_partitioned = spark.sql(\"\"\"\n",
    "    SELECT view, ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_sales_partitioned\n",
    "    GROUP BY view\n",
    "    HAVING avg_price >= 350000\n",
    "    ORDER BY view DESC\n",
    "\"\"\")\n",
    "print(\"Average price of a home per 'view' rating (partitioned, >= 350,000):\")\n",
    "avg_price_view_rating_partitioned.show()\n",
    "print(\"--- %s ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "hjjYzQGjtbq8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 14. Uncache the home_sales temporary table.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m spark\u001b[38;5;241m.\u001b[39mcatalog\u001b[38;5;241m.\u001b[39munchacheTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhome_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 14. Uncache the home_sales temporary table.\n",
    "spark.catalog.unchacheTable(\"home_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Sy9NBvO7tlmm"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 15. Check if the home_sales is no longer cached\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome_sales\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cached?\u001b[39m\u001b[38;5;124m\"\u001b[39m, spark\u001b[38;5;241m.\u001b[39mcatalog\u001b[38;5;241m.\u001b[39misCached(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhome_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 15. Check if the home_sales is no longer cached\n",
    "print(\"Is 'home_sales' cached?\", spark.catalog.isCached(\"home_sales\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Si-BNruRUGK3"
   },
   "outputs": [],
   "source": [
    "# Stop the Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
